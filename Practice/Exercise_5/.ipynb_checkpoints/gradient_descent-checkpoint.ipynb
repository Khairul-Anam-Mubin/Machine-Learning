{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fecc8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem : we know, for linear regression equation y = mx + b. given x , y. we have to find out m and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dbbac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ec70283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean_Square_Error(y , y_predicted):\n",
    "    sum = 0\n",
    "    n = len(y)\n",
    "    for i in range(len(y)):\n",
    "        sum += ((y[i] - y_predicted[i]) ** 2)\n",
    "    cost = (1 / n) * sum\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3475bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Descent(x , y):\n",
    "    m_cur = b_cur = 0\n",
    "    iterations = 100\n",
    "    n = len(x)\n",
    "    learning_rate = 0.08\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_cur * x + b_cur #as x is numpy m_cur * x will multiply all the elements in x\n",
    "        cost = Mean_Square_Error(y , y_predicted)\n",
    "        m_derivative = -(2 /n) * sum(x * (y - y_predicted))\n",
    "        b_derivative = -(2 /n) * sum(y - y_predicted)\n",
    "        m_cur = m_cur - learning_rate * m_derivative\n",
    "        b_cur = b_cur - learning_rate * b_derivative\n",
    "        print(\"iter = \", i + 1, \"m = \", m_cur, \" b = \", b_cur, \" loss = \" , cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c001613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter =  1 m =  4.96  b =  1.44  loss =  89.0\n",
      "iter =  2 m =  0.4991999999999983  b =  0.26879999999999993  loss =  71.10560000000002\n",
      "iter =  3 m =  4.451584000000002  b =  1.426176000000001  loss =  56.8297702400001\n",
      "iter =  4 m =  0.892231679999997  b =  0.5012275199999995  loss =  45.43965675929613\n",
      "iter =  5 m =  4.041314713600002  b =  1.432759910400001  loss =  36.35088701894832\n",
      "iter =  6 m =  1.2008760606719973  b =  0.7036872622079998  loss =  29.097483330142282\n",
      "iter =  7 m =  3.7095643080294423  b =  1.4546767911321612  loss =  23.307872849944438\n",
      "iter =  8 m =  1.4424862661541864  b =  0.881337636696883  loss =  18.685758762535738\n",
      "iter =  9 m =  3.4406683721083144  b =  1.4879302070713722  loss =  14.994867596913156\n",
      "iter =  10 m =  1.6308855378034224  b =  1.0383405553279617  loss =  12.046787238456794\n",
      "iter =  11 m =  3.2221235247119777  b =  1.5293810083298451  loss =  9.691269350698109\n",
      "iter =  12 m =  1.7770832372205707  b =  1.1780607551353204  loss =  7.8084968312098315\n",
      "iter =  13 m =  3.0439475772474127  b =  1.5765710804477953  loss =  6.302918117062937\n",
      "iter =  14 m =  1.8898457226770244  b =  1.3032248704973899  loss =  5.098330841763168\n",
      "iter =  15 m =  2.898169312926714  b =  1.6275829443328358  loss =  4.133961682056365\n",
      "iter =  16 m =  1.9761515088959358  b =  1.4160484030347593  loss =  3.361340532576948\n",
      "iter =  17 m =  2.7784216197824048  b =  1.6809279342791488  loss =  2.741808050753047\n",
      "iter =  18 m =  2.0415541605113807  b =  1.5183370872989306  loss =  2.244528230107478\n",
      "iter =  19 m =  2.6796170361078637  b =  1.735457156285639  loss =  1.8449036666988363\n",
      "iter =  20 m =  2.090471617540917  b =  1.611567833948162  loss =  1.5233119201782324\n",
      "iter =  21 m =  2.5976890103737853  b =  1.790290604096816  loss =  1.2640979056612756\n",
      "iter =  22 m =  2.1264168621494517  b =  1.6969533824619085  loss =  1.0547704368105268\n",
      "iter =  23 m =  2.529385561184701  b =  1.8447607474362664  loss =  0.8853615531285766\n",
      "iter =  24 m =  2.1521818147302194  b =  1.7754939584778073  loss =  0.7479156468369821\n",
      "iter =  25 m =  2.472104720735685  b =  1.8983676540508527  loss =  0.6360820885229722\n",
      "iter =  26 m =  2.1699839382964696  b =  1.8480185634495874  loss =  0.5447903801652151\n",
      "iter =  27 m =  2.423763296438881  b =  1.950743302915348  loss =  0.4699911136477278\n",
      "iter =  28 m =  2.1815831093070837  b =  1.9152179921582295  loss =  0.4084494012702221\n",
      "iter =  29 m =  2.3826922006906663  b =  2.0016232209455125  loss =  0.35758014655339476\n",
      "iter =  30 m =  2.1883747814212473  b =  1.9776712492627107  loss =  0.31531667795040486\n",
      "iter =  31 m =  2.3475529664737507  b =  2.0508239542984783  loss =  0.280005985849834\n",
      "iter =  32 m =  2.19146424741668  b =  2.0358666977033213  loss =  0.2503251729489924\n",
      "iter =  33 m =  2.317271157065729  b =  2.0982251873107836  loss =  0.2252148202231392\n",
      "iter =  34 m =  2.19172583072087  b =  2.0902190019495084  loss =  0.2038258415569305\n",
      "iter =  35 m =  2.2909832477163747  b =  2.1437555628915694  loss =  0.1854770944836773\n",
      "iter =  36 m =  2.1898500615476015  b =  2.1410827139250586  loss =  0.16962156815305135\n",
      "iter =  37 m =  2.2679942505397945  b =  2.1873814501542004  loss =  0.15581941113049289\n",
      "iter =  38 m =  2.1863812735157397  b =  2.188763177870427  loss =  0.14371641365577967\n",
      "iter =  39 m =  2.247743906750233  b =  2.2290980581236037  loss =  0.13302683968149862\n",
      "iter =  40 m =  2.181747562970493  b =  2.2335252935837153  loss =  0.1235197278285518\n",
      "iter =  41 m =  2.2297797112222417  b =  2.268922416384484  loss =  0.11500795886067308\n",
      "iter =  42 m =  2.176284659606544  b =  2.2756005683762908  loss =  0.1073395295828815\n",
      "iter =  43 m =  2.213735385878407  b =  2.306887840824943  loss =  0.10039058653754176\n",
      "iter =  44 m =  2.170254943136438  b =  2.315192801071317  loss =  0.09405986334903649\n",
      "iter =  45 m =  2.1993136987020745  b =  2.3430395801944157  loss =  0.08826423771269985\n",
      "iter =  46 m =  2.1638625904931037  b =  2.3524826719863134  loss =  0.08293518155053264\n",
      "iter =  47 m =  2.1862727486718105  b =  2.3774314010318136  loss =  0.07801592372722821\n",
      "iter =  48 m =  2.1572656385141533  b =  2.3876314575042543  loss =  0.07345918129710392\n",
      "iter =  49 m =  2.1744150151272015  b =  2.4101229178167802  loss =  0.06922534441882239\n",
      "iter =  50 m =  2.150585587951272  b =  2.4207840437050385  loss =  0.06528102333197575\n",
      "iter =  51 m =  2.1635786121786147  b =  2.441177514495622  loss =  0.06159788433500965\n",
      "iter =  52 m =  2.1439150477863547  b =  2.4520713783305874  loss =  0.05815171649232756\n",
      "iter =  53 m =  2.153630302083688  b =  2.470660734860243  loss =  0.054921682590988646\n",
      "iter =  54 m =  2.137323817683481  b =  2.4816124722824338  loss =  0.05188971727120564\n",
      "iter =  55 m =  2.1444599118649865  b =  2.4986390442291735  loss =  0.04904004275389065\n",
      "iter =  56 m =  2.1308637257526066  b =  2.509516039457312  loss =  0.04635877856867898\n",
      "iter =  57 m =  2.1359758694885094  b =  2.525178884782891  loss =  0.04383362645496491\n",
      "iter =  58 m =  2.124572474492945  b =  2.535881845863144  loss =  0.04145361541183607\n",
      "iter =  59 m =  2.128101633371053  b =  2.5503459627684273  loss =  0.03920889490609494\n",
      "iter =  60 m =  2.118476696509155  b =  2.5608018247073736  loss =  0.03709056666678448\n",
      "iter =  61 m =  2.120772834793503  b =  2.5742047184297996  loss =  0.03509054742420437\n",
      "iter =  62 m =  2.112594380710634  b =  2.5843610027801502  loss =  0.03320145649050715\n",
      "iter =  63 m =  2.1139349893254455  b =  2.5968179395942217  loss =  0.03141652330669783\n",
      "iter =  64 m =  2.1069367971074353  b =  2.606638274382932  loss =  0.0297295110602709\n",
      "iter =  65 m =  2.1075416624945413  b =  2.618246487870094  loss =  0.0281346532591438\n",
      "iter =  66 m =  2.1015100223265035  b =  2.6277070518134993  loss =  0.02662660077102551\n",
      "iter =  67 m =  2.101552998161378  b =  2.6385491128066176  loss =  0.025200377334927134\n",
      "iter =  68 m =  2.096316147250177  b =  2.6476358156400974  loss =  0.023851341948628327\n",
      "iter =  69 m =  2.095934536582619  b =  2.657782334457597  loss =  0.022575156852933542\n",
      "iter =  70 m =  2.0913542316575633  b =  2.6664885833847243  loss =  0.021367760086655644\n",
      "iter =  71 m =  2.090656263915584  b =  2.676000378847538  loss =  0.020225341788425083\n",
      "iter =  72 m =  2.0866210575773376  b =  2.6843253115524512  loss =  0.019144323582910155\n",
      "iter =  73 m =  2.0856918466960472  b =  2.693255154066937  loss =  0.018121340518098442\n",
      "iter =  74 m =  2.082111722558874  b =  2.7012022430021245  loss =  0.017153225123473996\n",
      "iter =  75 m =  2.0810180142142363  b =  2.709596257293525  loss =  0.01623699324146153\n",
      "iter =  76 m =  2.077820105696288  b =  2.717172209303728  loss =  0.015369831350564987\n",
      "iter =  77 m =  2.0766140592050317  b =  2.7250710050809133  loss =  0.014549085151541019\n",
      "iter =  78 m =  2.0737392325653374  b =  2.732284895849552  loss =  0.013772249230347736\n",
      "iter =  79 m =  2.0724614332425584  b =  2.7397244808822614  loss =  0.013036957645638886\n",
      "iter =  80 m =  2.0698615599121704  b =  2.7465870759846718  loss =  0.012340975315898037\n",
      "iter =  81 m =  2.0685434179941087  b =  2.7535995950692826  loss =  0.011682190103287127\n",
      "iter =  82 m =  2.066179196691222  b =  2.760122819221025  loss =  0.011058605508984853\n",
      "iter =  83 m =  2.064844857288579  b =  2.7667371537338745  loss =  0.010468333909073289\n",
      "iter =  84 m =  2.0626840746684203  b =  2.7729336776379365  loss =  0.009909590271584152\n",
      "iter =  85 m =  2.061351937985791  b =  2.7791759333750248  loss =  0.009380686304666458\n",
      "iter =  86 m =  2.0593680791107873  b =  2.785058853801841  loss =  0.00888002499345325\n",
      "iter =  87 m =  2.0580520100509183  b =  2.7909527592203687  loss =  0.00840609548939642\n",
      "iter =  88 m =  2.056223147935525  b =  2.7965353529206687  loss =  0.007957468320912865\n",
      "iter =  89 m =  2.05493343816708  b =  2.8021025854443096  loss =  0.007532790898352296\n",
      "iter =  90 m =  2.0532413459797505  b =  2.8073981214530215  loss =  0.007130783289727114\n",
      "iter =  91 m =  2.0519854787579397  b =  2.812658575950258  loss =  0.0067502342464980354\n",
      "iter =  92 m =  2.050414919687842  b =  2.8176801739944057  loss =  0.006389997461079277\n",
      "iter =  93 m =  2.0491981775199255  b =  2.8226521847051367  loss =  0.006048988039717134\n",
      "iter =  94 m =  2.047736336426391  b =  2.8274127099427506  loss =  0.005726179176078216\n",
      "iter =  95 m =  2.0465622835434223  b =  2.8321132348672426  loss =  0.005420599012302664\n",
      "iter =  96 m =  2.045198311770722  b =  2.836625221187641  loss =  0.005131327675503935\n",
      "iter =  97 m =  2.0440691768841837  b =  2.8410699961476715  loss =  0.0048574944787420265\n",
      "iter =  98 m =  2.042793827417138  b =  2.845345591859636  loss =  0.004598275276411798\n",
      "iter =  99 m =  2.0417108070703494  b =  2.8495492600018677  loss =  0.004352889964781892\n",
      "iter =  100 m =  2.0405161418256377  b =  2.8536001910078013  loss =  0.004120600119124239\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1 , 2 , 3 , 4 , 5])\n",
    "y = np.array([5 , 7 , 9 , 11 , 13])\n",
    "Gradient_Descent(x , y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
